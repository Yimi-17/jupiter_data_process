@article{0,
    key = {rayyan-4717607},
    title = {An Efficient Unsupervised Approach for OCR Error Correction of Vietnamese OCR Text},
    year = {2023},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 11},
    volume = {11},
    pages = {58406-58421},
    authors = {Q. -D. Nguyen and N. -M. Phan and P. Krömer and D. -A. Le},
    abstract = {Different types of OCR errors often occur in OCR texts due to the low quality of scanned document images or limitations in OCR software. In this paper, we propose a novel unsupervised approach for OCR error correction. Correction candidates for OCR errors are generated and explored in their neighborhoods using correction character edits controlled by an adapted hill-climbing algorithm. Correction characters are extracted from only original ground truth texts, which do not depend on OCR texts in training data. A weighted objective function used to score and rank correction candidates is heuristically tested to find optimal weight combinations. The proposed model is evaluated on an OCR text dataset originating from the Vietnamese handwritten database in the ICFHR 2018 Vietnamese online handwritten text recognition competition. The proposed model is also verified concerning its stability and complexity. The experimental results show that our model achieves competitive performance compared to the other models in the ICFHR 2018 competition.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2023.3283340},
    keywords = {Optical character recognition;Error correction;Computational modeling;Adaptation models;Optimization;Linguistics;Training data;Encoding;OCR;character edit;error correction;attention-based encoder-decoder;hill climbing},
}

@article{1,
    key = {rayyan-4717608},
    title = {MMU-OCR-21: Towards End-to-End Urdu Text Recognition Using Deep Learning},
    year = {2021},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 9},
    volume = {9},
    pages = {124945-124962},
    authors = {T. Nasir and M. K. Malik and K. Shahzad},
    abstract = {Optical Character Recognition (OCR) is a technique that generates text from an image. Recognizing the importance of OCR in real-world settings, a plethora of techniques have been developed for Western, as well as Asian languages. Urdu is a prominent South Asian language and a number of different solutions for Urdu OCR have been proposed. However, fewer attempts have been made to develop end-to-end deep learning-based solutions for recognizing printed Urdu text. Furthermore, several benchmark corpora for Urdu OCR have been developed that can be used for training and evaluation of different OCR techniques. However, there are a number of limitations of the existing Urdu corpora: firstly, most of them have either character or word or text images, which are usually rendered using only a single font, Nastaleeq. Secondly, the volume of the existing datasets is so small that it is not suitable for working with the deep-learning techniques that have achieved groundbreaking results for OCRs. To that end, in this study, we have proposed a very large Multi-level and Multi-script Urdu corpus (MMU-OCR-21). It is the largest-ever Urdu corpus of printed text that is effectively suitable to work with deep learning techniques. In total, the corpus is composed of over 602,472 images, including text-line and word images in three prominent fonts, and their respective ground truth. Also, we have performed experiments using multiple state-of-the-art deep learning techniques for text-line and word level images.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2021.3110787},
    keywords = {Optical character recognition software;Deep learning;Image segmentation;Feature extraction;Text recognition;Character recognition;Training;Artificial neural networks;corpus generation;image processing;optical character recognition;text recognition;Urdu OCR},
}

@article{2,
    key = {rayyan-4717609},
    title = {ADOCRNet: A Deep Learning OCR for Arabic Documents Recognition},
    year = {2024},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 12},
    volume = {12},
    pages = {55620-55631},
    authors = {L. Mosbah and I. Moalla and T. M. Hamdani and B. Neji and T. Beyrouthy and A. M. Alimi},
    abstract = {In recent years, Optical character recognition (OCR) has experienced a resurgence of interest especially for contemporary Arabic data. In fact, OCR development for printed and handwritten Arabic script is still a challenging task. These challenges are due to the specific characteristics of the Arabic script. In this work, we attempt to address these challenges by creating a deep learning OCR for Arabic document recognition called ADOCRNet. It is a novel deep learning framework whose architecture is built of layers of Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (BLSTM) trained using Connectionist Temporal Classification (CTC) algorithm. In order to assess the performance of our OCR, the proposed system is performed on two printed text datasets which are P-KHATT (text line images) and APTI (word images). It’s also evaluated on a handwritten Arabic text dataset IFN/ENIT (word images). According to the practical tests, the conceived model achieves strength recognition rates on the three datasets. ADOCRNet reaches a Character Error Rate (CER) of 0.01% on the P-KHATT dataset, 0.03% on the APTI dataset and a Word Error Rate (WER) of 1.09% on the IFN/ENIT dataset, which significantly outperforms the outcomes of the current systems.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2024.3379530},
    keywords = {Convolutional neural networks;Optical character recognition;Hidden Markov models;Character recognition;Text recognition;Handwriting recognition;Deep learning;Long Term Evolution;Bidirectional control;Arabic;document recognition;CNNs;CTC;deep learning;BLSTM;OCR},
}

@article{3,
    key = {rayyan-4717610},
    title = {An Enhanced Offline Printed Arabic OCR Model Based on Bio-Inspired Fuzzy Classifier},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {117770-117781},
    authors = {S. M. Darwish and K. O. Elzoghaly},
    abstract = {In the recent few years, there was a concentrated search on Arabic Optical Character Recognition (OCR), especially the recognition of scanned, offline, machine-printed documents. However, Arabic OCR consequences are dissatisfying and are still a developed research area. Finding the best feature extraction techniques and selecting an appropriate classification algorithm lead to supreme recognition accuracy and low computational overhead. This paper presents a new Arabic OCR model by integrating both of Genetic Algorithm (GA) and the Fuzzy K-Nearest Neighbor classifier (F-KNN) in a unified framework to enhance the identification accuracy. GA is utilized as a feature selection algorithm that has better convergence and spread of solutions with candid variation preservation mechanism. The F-KNN algorithm is more appropriate to classify ambiguous or uncertain data objects in the sense that every object belongs to all classes with different degrees of membership. The suggested model semantically fuses bio-inspired based feature vectors with fuzzy KNN classifier to build accurate membership function for each class. Experimental results compared to other approaches revealed the effectiveness of the suggested model and demonstrated that the feature selection approach increased the identification accuracy process.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.3004286},
    keywords = {Optical character recognition software;Feature extraction;Character recognition;Genetic algorithms;Biological system modeling;Image recognition;Image segmentation;Arabic OCR;fuzzy classification;feature selection;GA},
}

@article{4,
    key = {rayyan-4717611},
    title = {Handwritten Optical Character Recognition (OCR): A Comprehensive Systematic Literature Review (SLR)},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {142642-142668},
    authors = {J. Memon and M. Sami and R. A. Khan and M. Uddin},
    abstract = {Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.3012542},
    keywords = {Optical character recognition software;Character recognition;Databases;Optical imaging;Bibliographies;Protocols;Systematics;Optical character recognition;classification;languages;feature extraction;deep learning},
}

@article{5,
    key = {rayyan-4717612},
    title = {Efficient Text Bounding Box Identification Using Mask R-CNN: Case of Thai Documents},
    year = {2024},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 12},
    volume = {12},
    pages = {49306-49328},
    authors = {P. Kiatphaisansophon and D. Wanvarie and N. Cooharojananone},
    abstract = {Text detection is a fundamental task in computer vision, particularly for Optical Character Recognition (OCR) applications. This study focuses on text detection within an OCR application, encompassing text detection, text recognition, and information extraction, explicitly focusing on text detection. Character-Region Awareness for Text Detection (CRAFT), Pyramid Mask Text Detector (PMTD), and Scene Text Detection with Supervised Pyramid Context Network (SPCNET) have demonstrated promising results in bounding-box detection. However, it faces challenges related to post-processing and multiline text detection. A post-processing problem arises because of the need to reconfigure the model when new documents are introduced, which leads to inefficiencies and complexities. In addition, CRAFT tends to merge bounding boxes from consecutive lines by introducing multiline errors, especially for CRAFT. To address these challenges, this study proposes an adapted approach based on Mask R-CNN, an instance segmentation model that treats each text element as an individual object. By adopting the Mask R-CNN approach, post-processing issues were successfully eliminated. Moreover, the multiline problem is effectively resolved. Comparative experiments demonstrate that the proposed model achieves results comparable to those of these models while surpassing them in accuracy and versatility. The proposed model is extensively evaluated on various document types, including bankbooks, Thai ID cards (both front and back sides), invoices, car registrations, mobile banking slips, passports, Indonesian ID cards, driver licenses, and receipts. The results indicated the model’s high performance and potential for real-world applications. Eliminating post-processing and multiline problems ensures the model’s adaptability to a wide range of document structures and reduces both time inference and resource utilization.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2024.3383911},
    keywords = {Text detection;Feature extraction;Optical character recognition;Adaptation models;Object detection;Task analysis;Object recognition;Deep learning;text detection;optical character recognition (OCR)},
}

@article{6,
    key = {rayyan-4717614},
    title = {Handwritten Arabic Optical Character Recognition Approach Based on Hybrid Whale Optimization Algorithm With Neighborhood Rough Set},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {23011-23021},
    authors = {A. T. Sahlol and M. Abd Elaziz and M. A. A. Al-Qaness and S. Kim},
    abstract = {Accomplishing high recognition performance is considered one of the most important tasks for handwritten Arabic character recognition systems. In general, Optical Character Recognition (OCR) systems are constructed from four phases: pre-processing, feature extraction, feature selection, and classification. Recent literature focused on the selection of appropriate features as a key point towards building a successful and sufficient character recognition system. In this paper, we propose a hybrid machine learning approach that utilizes neighborhood rough sets with a binary whale optimization algorithm to select the most appropriate features for the recognition of handwritten Arabic characters. To validate the proposed approach, we used the CENPARMI dataset, which is a well-known dataset for machine learning experiments involving handwritten Arabic characters. The results show clear advantages of the proposed approach in terms of recognition accuracy, memory footprint, and processor time than those without the features of the proposed method. When comparing the results of the proposed method with other recent state-of-the-art optimization algorithms, the proposed approach outperformed all others in all experiments. Moreover, the proposed approach shows the highest recognition rate with the smallest consumption time compared to deep neural networks such as VGGnet, Resnet, Nasnet, Mobilenet, Inception, and Xception. The proposed approach was also compared with recently published works using the same dataset, which further confirmed the outstanding classification accuracy and time consumption of this approach. The misclassified failure cases were studied and analyzed, which showed that they would likely be confusing for even Arabic natives because the correct interpretation of the characters required the context of their appearance.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.2970438},
    keywords = {Feature extraction;Character recognition;Whales;Rough sets;Optimization;Optical character recognition software;Machine learning approach;feature selection;optimization;Arabic handwritten character recognition;whale optimization;neighborhood rough set;optical character recognition (OCR)},
}

@article{7,
    key = {rayyan-4717615},
    title = {A Neural Network Architecture for Information Extraction in Chinese Drug Package Insert},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {51256-51264},
    authors = {R. -G. Zhou and S. Chang and Y. Li},
    abstract = {There is a lot of useful information in the medical photocopying materials. The correct extraction and identification of this information are of great significance for the construction of digital medical. In most previous research, researchers have been working on clinical data, and there is little discussion on the extraction of information from Chinese drug package insert. To settle this issue, a neural network model is proposed in this paper. This model uses OCR's post-document as the data source, which can not only correct these data but also classify sentences. It is mainly composed of three layers: the first layer is employed to correct the data using the language model and the seq2seq model, the second layer is defined by convolution neural network (CNN) aiming to enrich the processed sentences, and another layer is used to determine the label of each sentence. The quantitative experimental results verify the feasibility and validity of the proposed model. In addition, the comparing experiments demonstrate that our method outperforms the regular rule-based approaches, which indicated 4%-6% higher in F1 score.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.2978079},
    keywords = {Optical character recognition software;Data models;Error correction;Neural networks;Drugs;Feature extraction;Probability;Chinese medical photocopying;neural network;OCR post correction;seq2seq model;sentence classification;convolutional neural network},
}

@article{8,
    key = {rayyan-4717617},
    title = {Urdu-Text Detection and Recognition in Natural Scene Images Using Deep Learning},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {96787-96803},
    authors = {S. Y. Arafat and M. J. Iqbal},
    abstract = {Urdu text is a cursive script and belongs to a non-Latin family of other cursive scripts like Arabic, Chinese, and Hindi. Urdu text poses a challenge for detection/localization from natural scene images, and consequently recognition of individual ligatures in scene images. In this paper, a methodology is proposed that covers detection, orientation prediction, and recognition of Urdu ligatures in outdoor images. As a first step, the custom FasterRCNN algorithm has been used in conjunction with well-known CNNs like Squeezenet, Googlenet, Resnet18, and Resnet50 for detection and localization purposes for images of size 320 × 240 pixels. For ligature Orientation prediction, a custom Regression Residual Neural Network (RRNN) is trained/tested on datasets containing randomly oriented ligatures. Recognition of ligatures was done using Two Stream Deep Neural Network (TSDNN). In our experiments, five-set of datasets, containing 4.2K and 51K Urdu-text-embedded synthetic images were generated using the CLE annotation text to evaluate different tasks of detection, orientation prediction, and recognition of ligatures. These synthetic images contain 132, and 1600 unique ligatures corresponding to 4.2K and 51K images respectively, with 32 variations of each ligature (4-backgrounds and font 8-color variations). Also, 1094 real-world images containing more than 12k Urdu characters were used for TSDNN's evaluation. Finally, all four detectors were evaluated and used to compare them for their ability to detect/localize Urdu-text using average-precision (AP). Resnet50 features based FasterRCNN was found to be the winner detector with AP of.98. While Squeeznet, Googlenet, Resnet18 based detectors had testing AP of.65, .88, and .87 respectively. RRNN achieved and accuracy of 79% and 99% for 4k and 51K images respectively. Similarly, for characters classification in ligatures, TSDNN attained a partial sequence recognition rate of 94.90% and 95.20% for 4k and 51K images respectively. Similarly, a partial sequence recognition rate of 76.60% attained for real world-images.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.2994214},
    keywords = {Text recognition;Image recognition;Character recognition;Neural networks;Feature extraction;Machine learning;Streaming media;BLSTM;deep neural network;FasterRCNN;image classification;Nastalique;optical character recognition (OCR);regression residual neural network (RRNN);synthetic urdu text;text detection;two stream deep neural network (TSDNN)},
}

@article{9,
    key = {rayyan-4717618},
    title = {CArDIS: A Swedish Historical Handwritten Character and Word Dataset},
    year = {2022},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 10},
    volume = {10},
    pages = {55338-55349},
    authors = {A. Yavariabdi and H. Kusetogullari and T. Celik and S. Thummanapally and S. Rijwan and J. Hall},
    abstract = {This paper introduces a new publicly available image-based Swedish historical handwritten character and word dataset named Character Arkiv Digital Sweden (CArDIS) (https://cardisdataset.github.io/CARDIS/). The samples in CArDIS are collected from 64, 084 Swedish historical documents written by several anonymous priests between 1800 and 1900. The dataset contains 116, 000 Swedish alphabet images in RGB color space with 29 classes, whereas the word dataset contains 30, 000 image samples of ten popular Swedish names as well as 1, 000 region names in Sweden. To examine the performance of different machine learning classifiers on CArDIS dataset, three different experiments are conducted. In the first experiment, classifiers such as Support Vector Machine (SVM), Artificial Neural Networks (ANN), k-Nearest Neighbor (k-NN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Random Forest (RF) are trained on existing character datasets which are Extended Modified National Institute of Standards and Technology (EMNIST), IAM and CVL and tested on CArDIS dataset. In the second and third experiments, the same classifiers as well as two pre-trained VGG-16 and VGG-19 classifiers are trained and tested on CArDIS character and word datasets. The experiments show that the machine learning methods trained on existing handwritten character datasets struggle to recognize characters efficiently on the CArDIS dataset, proving that characters in the CArDIS contain unique features and characteristics. Moreover, in the last two experiments, the deep learning-based classifiers provide the best recognition rates.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2022.3175197},
    keywords = {Character recognition;Optical character recognition software;Feature extraction;Hidden Markov models;Handwriting recognition;Machine learning;Image recognition;Character and word recognition;machine learning methods;optical character recognition (OCR);old handwritten style;Swedish handwritten character dataset;Swedish handwritten word dataset},
}

@article{10,
    key = {rayyan-4717619},
    title = {Efficient Automated Processing of the Unstructured Documents Using Artificial Intelligence: A Systematic Literature Review and Future Directions},
    year = {2021},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 9},
    volume = {9},
    pages = {72894-72936},
    authors = {D. Baviskar and S. Ahirrao and V. Potdar and K. Kotecha},
    abstract = {The unstructured data impacts 95% of the organizations and costs them millions of dollars annually. If managed well, it can significantly improve business productivity. The traditional information extraction techniques are limited in their functionality, but AI-based techniques can provide a better solution. A thorough investigation of AI-based techniques for automatic information extraction from unstructured documents is missing in the literature. The purpose of this Systematic Literature Review (SLR) is to recognize, and analyze research on the techniques used for automatic information extraction from unstructured documents and to provide directions for future research. The SLR guidelines proposed by Kitchenham and Charters were adhered to conduct a literature search on various databases between 2010 and 2020. We found that: 1. The existing information extraction techniques are template-based or rule-based, 2. The existing methods lack the capability to tackle complex document layouts in real-time situations such as invoices and purchase orders, 3. The datasets available publicly are task-specific and of low quality. Hence, there is a need to develop a new dataset that reflects real-world problems. Our SLR discovered that AI-based approaches have a strong potential to extract useful information from unstructured documents automatically. However, they face certain challenges in processing multiple layouts of the unstructured documents. Our SLR brings out conceptualization of a framework for construction of high-quality unstructured documents dataset with strong data validation techniques for automated information extraction. Our SLR also reveals a need for a close association between the businesses and researchers to handle various challenges of the unstructured data analysis.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2021.3072900},
    keywords = {Organizations;Information retrieval;Optical character recognition software;Data mining;Automation;Standards organizations;Bibliographies;Artificial Intelligence (AI);document analysis;information extraction;named entity recognition (NER);optical character recognition (OCR);robotics process automation (RPA);unstructured data},
}

@article{11,
    key = {rayyan-4717620},
    title = {A Text Detection Algorithm for Image of Student Exercises Based on CTPN and Enhanced YOLOv3},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {176924-176934},
    authors = {L. Cao and H. Li and R. Xie and J. Zhu},
    abstract = {Intelligent learning system (ILS) has become a popular learning tool for students. It can collect students' wrong questions in exercises and dig out their unskilled knowledge points so that it can recommend personalized exercises for students. Detecting text accurately from images of students' exercises is significant and essential in an ILS. However, a big challenge of text detection is that traditional text detection algorithms can not detect complete text lines in an exercise scene, and their detection box always splits between Chinese and mathematical symbols. In this article, we propose a deep-learning-based approach for text detection, which improves You Only Look Once version 3 (YOLOv3) by changing the regression object from a single character to a fixed-width text and applies a stitching strategy to construct text lines based on the relation matrix, which improves the accuracy by 9.8%. Experimental results on both RCTW Chinese text detection dataset and real exercise scenario show that our model can improve detection effectiveness. In addition, we compare our method with two state-of-the-art approaches in applications of exercise text detection, and discuss its capability and limitations. We have also provided a platform which has implemented the proposal for detecting text lines in students' daily homework or examination papers, which enhances user experience well.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.3025221},
    keywords = {Text recognition;Databases;Splicing;Detectors;Feature extraction;Object detection;Detection algorithms;Text detection;exercise image;YOLOv3;CTPN;OCR platform;stitching algorithm},
}

@article{12,
    key = {rayyan-4717621},
    title = {Quranic Optical Text Recognition Using Deep Learning Models},
    year = {2021},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 9},
    volume = {9},
    pages = {38318-38330},
    authors = {M. Mohd and F. Qamar and I. Al-Sheikh and R. Salah},
    abstract = {A Quranic optical character recognition (OCR) system based on convolutional neural network (CNN) followed by recurrent neural network (RNN) is introduced in this work. Six deep learning models are built to study the effect of different representations of the input and output, and the accuracy and performance of the models, and compare long short-term memory (LSTM) and gated recurrent unit (GRU). A new Quranic OCR dataset is developed based on the most famous printed version of the Holy Quran (Mushaf Al-Madinah), and a page and line-text image with the corresponding labels is prepared. This work’s contribution is a Quranic OCR model capable of recognizing the Quranic image’s diacritic text. A better performance in word recognition rate (WRR) and character recognition rate (CRR) is achieved in the experiments. The LSTM and GRU are compared in the Arabic text recognition domain. In addition, a public database is built for research purposes in Arabic text recognition that contains the diacritics and the Uthmanic script, and is large enough to be used with the deep learning models. The outcome of this work shows that the proposed system obtains an accuracy of 98% on the validation data, and a WRR of 95% and a CRR of 99% in the test dataset.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2021.3064019},
    keywords = {Text recognition;Optical character recognition software;Deep learning;Character recognition;Writing;Hidden Markov models;Arabic;Quranic text;deep learning;gated recurrent unit;long short-term memory;optical character recognition},
}

@article{13,
    key = {rayyan-4717623},
    title = {Switching Text-Based Image Encoders for Captioning Images With Text},
    year = {2023},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 11},
    volume = {11},
    pages = {55706-55715},
    authors = {A. Ueda and W. Yang and K. Sugiura},
    abstract = {Visual understanding, such as image caption generation, has received extensive attention. Describing images with textual information is one way to help people achieve barrier-free visibility. This study focuses on the text-based image captioning (TextCaps) task. The TextCaps task is more complex than the traditional image captioning task because it depends on optical character recognition (OCR) and the textual information that appears in the image. It also requires consideration of the relationship between recognized objects and OCR’s linguistic part in the image. In this study, we propose maximizing the use of multiple modalities in an image to improve TextCaps performance. We enrich the image and OCR linguistic features using pre-trained Contrastive Language-Image Pre-training (CLIP) models. We then introduce using two additional attention models in a transformer architecture to strengthen the representation of the image modality. The experimental results demonstrate that our proposed method, which introduces a multimodal transformer with four image-related modalities, outperforms existing methods for the TextCaps dataset.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2023.3282444},
    keywords = {Optical character recognition;Task analysis;Feature extraction;Visualization;Transformers;Text recognition;Image recognition;Multimodal text-based image captioning;transformer attention mechanism;text-based image encoders},
}

@article{14,
    key = {rayyan-4717624},
    title = {PHTI: Pashto Handwritten Text Imagebase for Deep Learning Applications},
    year = {2022},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 10},
    volume = {10},
    pages = {113149-113157},
    authors = {I. Hussain and R. Ahmad and S. Muhammad and K. Ullah and H. Shah and A. Namoun},
    abstract = {Document Image Analysis (DIA) is one of the research areas of Artificial Intelligence (AI) that converts document images into machine-readable codes. In DIA systems, Optical Character Recognition (OCR) plays a key role in digitizing document images. The output of an OCR system is further used in many applications including, Natural Language Processing (NLP), Sentiment Analysis, Speech Recognition, and Translation Services. However, standard datasets are an essential requirement for the development, evaluation and comparison of different text recognition techniques. Pashto is one of such low resource languages that lacks availability regarding standard dataset of handwritten text. This paper therefore, addresses the unavailability of standard dataset for the Pashto handwritten text by developing a dataset named Pashto Handwritten Text Imagebase (PHTI). The PHTI is created by collecting handwritten samples from diverse genre of the Pashto language including poetry, religion, short stories, articles, novels, sports, culture and news. The dataset consists of 4,000 scanned images, written by 400 writers including 200 males and 200 females. These 4,000 images are further segmented into 36,082 text-line images. Each text-line image is annotated/ transcribed with UTF-8 codecs. The dataset can be used for many deep learning-based applications including, text recognition, skew detection, gender classification and age-groups classification.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2022.3216881},
    keywords = {Optical character recognition;Text recognition;Handwriting recognition;Artificial intelligence;Writing;Text analysis;Image segmentation;Natural language processing;Speech recognition;document image analysis;handwritten text;natural language processing;optical character recognition;speech recognition;Pashto;standard dataset},
}

@article{15,
    key = {rayyan-4717626},
    title = {Advancing Multilingual Handwritten Numeral Recognition With Attention-Driven Transfer Learning},
    year = {2024},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 12},
    volume = {12},
    pages = {41381-41395},
    authors = {A. Fateh and R. T. Birgani and M. Fateh and V. Abolghasemi},
    abstract = {As deep learning continues to evolve, we have observed huge breakthroughs in the fields of medical imaging, video and frame generation, optical character recognition (OCR), and other domains. In the field of data analysis and document processing, the recognition of handwritten numerals plays a crucial role. This work has led to remarkable changes in OCR, historical handwritten document analysis, and postal automation. In this study, we present a novel framework to overcome this challenge, going beyond digit recognition in only one language. Unlike common methods that focus on a limited set of languages, our method provides a comprehensive solution for recognition of handwritten digit images in 12 different languages. These specific languages are chosen because most of them have fairly distant representations in latent space. We utilize transfer learning, as it reduces the computational cost and maintains the quality of enhanced images and the models’ recognition accuracy. Another strength of our approach is the innovative attention-based module called the MRA module. Our experiments confirm that by applying this module, major progress is made in both image quality and the accuracy of handwritten digit recognition. Notably, we reached high precisions, surpassing nearly 2% improvement in specific languages compared to earlier techniques. In this work, we present a robust and cost-effective approach that handles multilingual handwritten numeral recognition across a wide range of languages. The code and further implementation details are available at https://github.com/CVLab-SHUT/HandWrittenDigitRecognition.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2024.3378598},
    keywords = {Handwriting recognition;Transfer learning;Image recognition;Support vector machines;Deep learning;Computational modeling;Training;transfer learning;multilingual;handwritten numeral recognition},
}

@article{16,
    key = {rayyan-4717627},
    title = {Multi-Lingual Optical Character Recognition System Using the Reinforcement Learning of Character Segmenter},
    year = {2020},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 8},
    volume = {8},
    pages = {174437-174448},
    authors = {J. Park and E. Lee and Y. Kim and I. Kang and H. I. Koo and N. I. Cho},
    abstract = {In this article, we present a new multi-lingual Optical Character Recognition (OCR) system for scanned documents. In the case of Latin characters, current open source systems such as Tesseract provide very high accuracy. However, the accuracy of the multi-lingual documents, including Asian characters, is usually lower than that for Latin-only documents. For example, when the document is the mix of English, Chinese and/or Korean characters, the OCR accuracy is lowered than English-only because the character/text properties of Chinese and Korean are quite different from Latin-type characters. To tackle these problems, we propose a new framework using three neural blocks (a segmenter, a switcher, and multiple recognizers) and the reinforcement learning of the segmenter: The segmenter partitions a given word image into multiple character images, the switcher assigns a recognizer for each sub-image, and the recognizers perform the recognition of assigned sub-images. The training of recognizers and switcher can be considered traditional image classification tasks and we train them with a supervised learning method. However, the supervised learning of the segmenter has two critical drawbacks: Its objective function is sub-optimal and its training requires a large amount of annotation efforts. Thus, by adopting the REINFORCE algorithm, we train the segmenter so as to optimize the overall performance, i.e., we minimize the edit distance of final recognition results. Experimental results have shown that the proposed method significantly improves the performance for multi-lingual scripts and large character set languages without using character boundary labels.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2020.3025769},
    keywords = {Image segmentation;Character recognition;Optical character recognition software;Image recognition;Optical switches;Learning (artificial intelligence);Deep learning;document analysis;optical character recognition},
}

@article{17,
    key = {rayyan-4717631},
    title = {Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks},
    year = {2021},
    journal = {IEEE Access},
    issn = {2169-3536     VO  - 9},
    volume = {9},
    pages = {101065-101077},
    authors = {I. O. De Oliveira and R. Laroca and D. Menotti and K. V. O. Fonseca and R. Minetto},
    abstract = {This work addresses the problem of vehicle identification through non-overlapping cameras. As our main contribution, we introduce a novel dataset for vehicle identification, called Vehicle-Rear, that contains more than three hours of high-resolution videos, with accurate information about the make, model, color and year of nearly 3,000 vehicles, in addition to the position and identification of their license plates. To explore our dataset we design a two-stream Convolutional Neural Network (CNN) that simultaneously uses two of the most distinctive and persistent features available: the vehicle’s appearance and its license plate. This is an attempt to tackle a major problem: false alarms caused by vehicles with similar designs or by very close license plate identifiers. In the first network stream, shape similarities are identified by a Siamese CNN that uses a pair of low-resolution vehicle patches recorded by two different cameras. In the second stream, we use a CNN for Optical Character Recognition (OCR) to extract textual information, confidence scores, and string similarities from a pair of high-resolution license plate patches. Then, features from both streams are merged by a sequence of fully connected layers for decision. In our experiments, we compared the two-stream network against several well-known CNN architectures using single or multiple vehicle features. The architectures, trained models, and dataset are publicly available at https://github.com/icarofua/vehicle-rear.},
    notes = {RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2021.3097964},
    keywords = {License plate recognition;Cameras;Feature extraction;Image recognition;Optical character recognition software;Videos;Road traffic;Character recognition;Vehicle identification;vehicle matching;multi-stream neural networks;feature fusion},
}

@article{18,
    key = {rayyan-4717650},
    title = {A Survey of OCR in Arabic Language: Applications, Techniques, and Challenges},
    year = {2023},
    journal = {APPLIED SCIENCES-BASEL},
    issn = {2076-3417     J9  - APPL SCI-BASEL     JI  - Appl. Sci.-Basel},
    volume = {13},
    authors = {Faizullah, S and Ayub, MS and Hussain, S and Khan, MA},
    abstract = {Optical character recognition (OCR) is the process of extracting handwritten or printed text from a scanned or printed image and converting it to a machine-readable form for further data processing, such as searching or editing. Automatic text extraction using OCR helps to digitize documents for improved productivity and accessibility and for preservation of historical documents. This paper provides a survey of the current state-of-the-art applications, techniques, and challenges in Arabic OCR. We present the existing methods for each step of the complete OCR process to identify the best-performing approach for improved results. This paper follows the keyword-search method for reviewing the articles related to Arabic OCR, including the backward and forward citations of the article. In addition to state-of-art techniques, this paper identifies research gaps and presents future directions for Arabic OCR.},
    notes = {Times Cited in Web of Science Core Collection:  6     Total Times Cited:  6     Cited Reference Count:  109 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.3390/app13074584     WE  - Science Citation Index Expanded (SCI-EXPANDED)},
    keywords = {optical character recognition;Arabic OCR;preprocessing;segmentation;classification;postprocessing;OPTICAL CHARACTER-RECOGNITION;TEXT LINE SEGMENTATION;DOCUMENT IMAGES;TEST FRAMEWORK;SKEW DETECTION},
}

@article{19,
    key = {rayyan-4717651},
    title = {Arabic Optical Character Recognition: A Review},
    year = {2023},
    journal = {CMES-COMPUTER MODELING IN ENGINEERING & SCIENCES},
    issn = {["1526-1492", "1526-1506     J9  - CMES-COMP MODEL ENG     JI  - CMES-Comp. Model. Eng. Sci."]},
    volume = {135},
    pages = {1825-1861},
    authors = {Alghyaline, S},
    abstract = {This study aims to review the latest contributions in Arabic Optical Character Recognition (OCR) during the last decade, which helps interested researchers know the existing techniques and extend or adapt them accordingly. The study describes the characteristics of the Arabic language, different types of OCR systems, different stages of the Arabic OCR system, the researcher's contributions in each step, and the evaluation metrics for OCR. The study reviews the existing datasets for the Arabic OCR and their characteristics. Additionally, this study implemented some preprocessing and segmentation stages of Arabic OCR. The study compares the performance of the existing methods in terms of recognition accuracy. In addition to researchers' OCR methods, commercial and open-source systems are used in the comparison. The Arabic language is morphologically rich and written cursive with dots and diacritics above and under the characters. Most of the existing approaches in the literature were evaluated on isolated characters or isolated words under a controlled environment, and few approaches were tested on page-level scripts. Some comparative studies show that the accuracy of the existing Arabic OCR commercial systems is low, under 75% for printed text, and further improvement is needed. Moreover, most of the current approaches are offline OCR systems, and there is no remarkable contribution to online OCR systems.},
    notes = {Times Cited in Web of Science Core Collection:  4     Total Times Cited:  4     Cited Reference Count:  143 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.32604/cmes.2022.024555     WE  - Science Citation Index Expanded (SCI-EXPANDED)},
    keywords = {Arabic Optical Character Recognition (OCR);Arabic OCR software;Arabic OCR datasets;Arabic OCR evaluation;SEGMENTATION-FREE;TEXT RECOGNITION;TRANSFORM;SCRIPTS;SYSTEM;ROBUST;MODEL},
}

@article{20,
    key = {rayyan-4717652},
    title = {Advancing OCR Accuracy in Image-to-LaTeX Conversion-A Critical and Creative Exploration},
    year = {2023},
    journal = {APPLIED SCIENCES-BASEL},
    issn = {2076-3417     J9  - APPL SCI-BASEL     JI  - Appl. Sci.-Basel},
    volume = {13},
    authors = {Orji, EZ and Haydar, A and Ersan, I and Mwambe, OO},
    abstract = {This paper comprehensively assesses the application of active learning strategies to enhance natural language processing-based optical character recognition (OCR) models for image-to-LaTeX conversion. It addresses the existing limitations of OCR models and proposes innovative practices to strengthen their accuracy. Key components of this study include the augmentation of training data with LaTeX syntax constraints, the integration of active learning strategies, and the employment of active learning feedback loops. This paper first examines the current weaknesses of OCR models with a particular focus on symbol recognition, complex equation handling, and noise moderation. These limitations serve as a framework against which the subsequent research methodologies are assessed. Augmenting the training data with LaTeX syntax constraints is a crucial strategy for improving model precision. Incorporating symbol relationships, wherein contextual information is considered during recognition, further enriches the error correction. This paper critically examines the application of active learning strategies. The active learning feedback loop leads to progressive improvements in accuracy. This article underlines the importance of uncertainty and diversity sampling in sample selection, ensuring that the dynamic learning process remains efficient and effective. Appropriate evaluation metrics and ensemble techniques are used to improve the operational learning effectiveness of the OCR model. These techniques allow the model to adapt and perform more effectively in diverse application domains, further extending its utility.},
    notes = {Times Cited in Web of Science Core Collection:  0     Total Times Cited:  0     Cited Reference Count:  71 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.3390/app132212503     WE  - Science Citation Index Expanded (SCI-EXPANDED)},
    keywords = {optical character recognition (OCR);LaTeX;active learning strategies;image-to-LaTeX conversion;natural language processing (NLP)},
}

@article{21,
    key = {rayyan-4717653},
    title = {Computational modelling of an optical character recognition system for <i>Yoruba</i> printed text images},
    year = {2020},
    journal = {SCIENTIFIC AFRICAN},
    issn = {2468-2276     J9  - SCI AFR     JI  - Sci. Afr.},
    volume = {9},
    authors = {Oni, OJ and Asahiah, FO},
    abstract = {This study acquired a dataset of scanned images of Standard Yoruba printed text and formulated a Yoruba character image recognition model. The model formulated was implemented and the performance of the model evaluated to develop an Optical Character Recognition (OCR) model for Yoruba printed text images.        The image dataset at 300 dots per inches (dpi) was acquired by generating image text-line from Yoruba New Testament Bible (Bibeli Mimo) corpus using Unicode UTF8. The Long Short Term Memory (LSTM) model, a variant of Recurrent Neural Network (RNN) was used to formulate the Standard Yoruba character image recognition model. The Python OCRopus framework was used to implement the model designed. The performance of the model designed was evaluated using character error rate based on Levenshtein Edit Distance algorithm.        The results show that the Character Error Rate (CER) of 3.138% for the font Times New Roman which gives better recognition than the other font style metric performance. The model achieved an OCR result of (7.435% CER) DejaVuSans font style image dataset, while for Ariel font image dataset, a result of 15.141% was achieved. The introduction of Language model-based Standard Yoruba a spell-checker corrector show a reduction in the Character Error Rate. The Times New Roman font recorded an error rate of 1.182%, the DejaVuSans font style at an error rate of 4.098% while the Ariel font at 5.87%.        The study concluded that the performance of the model shows that the farther away an image text font is from the font(s) used in training the network, the higher the character error rate of the recognition and that the inclusion of a post-processing stage shows a reduction in the Character Error Rates. (C) 2020 The Author(s). Published by Elsevier B.V. on behalf of African Institute of Mathematical Sciences / Next Einstein Initiative.},
    notes = {Times Cited in Web of Science Core Collection:  3     Total Times Cited:  4     Cited Reference Count:  22 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1016/j.sciaf.2020.e00415     WE  - Emerging Sources Citation Index (ESCI)},
    keywords = {Optical character;Yoruba;Orthography;Computational modelling;Spell-Check correction;OCRopus},
}

@article{22,
    key = {rayyan-4717655},
    title = {A Review of Document Binarization: Main Techniques, New Challenges, and Trends},
    year = {2024},
    journal = {ELECTRONICS},
    issn = {2079-9292     J9  - ELECTRONICS-SWITZ     JI  - Electronics},
    volume = {13},
    authors = {Yang, ZX and Zuo, SK and Zhou, YX and He, JL and Shi, JW},
    abstract = {Document image binarization is a challenging task, especially when it comes to text segmentation in degraded document images. The binarization, as a pre-processing step of Optical Character Recognition (OCR), is one of the most fundamental and commonly used segmentation methods. It separates the foreground text from the background of the document image to facilitate subsequent image processing. In view of the different degradation degrees of document images, researchers have proposed a variety of solutions. In this paper, we have summarized some challenges and difficulties in the field of document image binarization. Approximately 60 methods documenting image binarization techniques are mentioned, including traditional algorithms and deep learning-based algorithms. Here, we evaluated the performance of 25 image binarization techniques on the H-DIBCO2016 dataset to provide some help for future research.},
    notes = {Times Cited in Web of Science Core Collection:  0     Total Times Cited:  0     Cited Reference Count:  124 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.3390/electronics13071394     WE  - Science Citation Index Expanded (SCI-EXPANDED)},
    keywords = {degraded document images;binarization;threshold processing;deep learning;THRESHOLD SELECTION METHOD;IMAGE BINARIZATION;NETWORK;COMBINATION;ALGORITHM;TEXT},
}

@article{23,
    key = {rayyan-4717656},
    title = {Analysis of Recent Deep Learning Techniques for Arabic Handwritten-Text OCR and Post-OCR Correction},
    year = {2023},
    journal = {APPLIED SCIENCES-BASEL},
    issn = {2076-3417     J9  - APPL SCI-BASEL     JI  - Appl. Sci.-Basel},
    volume = {13},
    authors = {Najam, R and Faizullah, S},
    abstract = {Arabic handwritten-text recognition applies an OCR technique and then a text-correction technique to extract the text within an image correctly. Deep learning is a current paradigm utilized in OCR techniques. However, no study investigated or critically analyzed recent deep-learning techniques used for Arabic handwritten OCR and text correction during the period of 2020-2023. This analysis fills this noticeable gap in the literature, uncovering recent developments and their limitations for researchers, practitioners, and interested readers. The results reveal that CNN-LSTM-CTC is the most suitable architecture among Transformer and GANs for OCR because it is less complex and can hold long textual dependencies. For OCR text correction, applying DL models to generated errors in datasets improved accuracy in many works. In conclusion, Arabic OCR has the potential to further apply several text-embedding models to correct the resultant text from the OCR, and there is a significant gap in studies investigating this problem. In addition, there is a need for more high-quality and domain-specific OCR Arabic handwritten datasets. Moreover, we recommend the practical development of a space for future trends in Arabic OCR applications, derived from current limitations in Arabic OCR works and from applications in other languages; this will involve a plethora of possibilities that have not been effectively researched at the time of writing.},
    notes = {Times Cited in Web of Science Core Collection:  3     Total Times Cited:  3     Cited Reference Count:  83 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.3390/app13137568     WE  - Science Citation Index Expanded (SCI-EXPANDED)},
    keywords = {deep learning;Arabic handwritten text recognition;optical character recognition;post-OCR correction;Arabic OCR;LSTM;CTC;Transformer;GANs;Arabic diacritization},
}

@article{24,
    key = {rayyan-4717658},
    title = {Machine Learning in Computer Vision: A Review},
    year = {2021},
    journal = {EAI ENDORSED TRANSACTIONS ON SCALABLE INFORMATION SYSTEMS},
    issn = {2032-9407     J9  - EAI ENDORSED TRANS S     JI  - EAI Endorsed Trans. Scalable Inform. Syst.},
    volume = {8},
    authors = {Khan, AA and Laghari, AA and Awan, SA},
    abstract = {INTRODUCTION: Due to the advancement in the field of Artificial Intelligence (AI), the ability to tackle entire problems of machine intelligence. Nowadays, Machine learning (ML) is becoming a hot topic due to the direct training of machines with less interaction with a human. The scenario of manual feeding of the machine is changed in the modern era, it will learn automatically. Supervised and unsupervised ML techniques are used as a distinct purpose like feature extraction, pattern recognition, object detection, and classification.        OBJECTIVES: In Computer Vision (CV), ML performs a significant role to extract crucial information from images. CV successfully contributes to multiple domains, surveillance system, optical character recognition, robotics, suspect detection, and many more. The direction of CV research is going toward healthcare realm, medical imaging (MI) is the emerging technology, play a vital role to enhance image quality and recognized critical features of binary medical image, covert original image into grayscale and set the threshold values for segmentation.        CONTRIBUTION: This paper will address the importance of machine learning, state-of-the-art, and how ML is utilized in computer vision and image processing. This survey will provide details about the type of tools and applications, datasets, and techniques. Limitations of previous work and challenges of future work also discussed. Further, we identify and discuss a set of open issues yet to be addressed, for efficiently applying of ML in Computer vision and image process.        METHODS, RESULTS, AND CONCLUSION: In this review paper, we have discussed the techniques and various types of supervised and unsupervised algorithms of ML, general overview of image processing and the results based on the impact; neural network enabled models, limitations, tools and application of CV, moreover, highlight the critical open research areas of ML in CV.},
    notes = {Times Cited in Web of Science Core Collection:  39     Total Times Cited:  40     Cited Reference Count:  95 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.4108/eai.21-4-2021.169418     WE  - Emerging Sources Citation Index (ESCI)},
    keywords = {Machine Learning;Computer Vision;Supervised and Unsupervised Learning;Medical Imaging;Pattern Recognition;Feature Extraction;Neural Network;UNSUPERVISED CHANGE DETECTION;VEHICLE DETECTION;IMAGE;SEGMENTATION;INTELLIGENCE;ALGORITHMS;NETWORK},
}

@article{25,
    key = {rayyan-4717659},
    title = {Towards Assisting the Visually Impaired: A Review on Techniques for Decoding the Visual Data From Chart Images},
    year = {2021},
    journal = {IEEE ACCESS},
    issn = {2169-3536     J9  - IEEE ACCESS     JI  - IEEE Access},
    volume = {9},
    pages = {52926-52943},
    authors = {Shahira, KC and Lijiya, A},
    abstract = {The textual data of a document is supplemented by the graphical information in it. To make communication easier, they contain tables, charts and images. However, it excludes a section of our population - the visually impaired. With technological advancements, the blind can access the documents through text to speech software solutions. In this method, even images can be conveyed by reading out the figure captions. However, charts and other statistical comparisons which involve critical information are difficult to be "read" out this way. Aim of this paper is to analyse various methods available to solve this vexatious issue. We survey the state-of-the-art works that do the exact opposite of graphing tools. In this paper, we explore the existing literature in understanding the graphs and extracting the visual encoding from them. We classify these approaches into modality-based approaches, conventional and deep-learning based methods. The survey also contains comparisons and analyses relevant study datasets. As an outcome of this survey, we observe that: (i) All existing works under each category need decoding in a variety of graphs. (ii) Among the approaches, deep learning performs remarkably well in localisation and classification. However, it needs further improvements in reasoning from chart images. (iii) Research works are still in progress to access data from vector images. Recreating data from the raster images has unresolved issues. Based on this study, the various applications of decoding the graphs, challenges and future possibilities are also discussed. This paper explores current works in the extraction of chart data, which seek to enable researchers in Human Computer Interaction to achieve human-level perception of visual data by machines. In this era of visual summarisation of data, the AI approaches can automate the underlying data extraction and hence provide the natural language descriptions to support visually disabled users.},
    notes = {Times Cited in Web of Science Core Collection:  10     Total Times Cited:  11     Cited Reference Count:  108 | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.1109/ACCESS.2021.3069205     WE  - Science Citation Index Expanded (SCI-EXPANDED)},
    keywords = {Visualization;Image color analysis;Optical character recognition software;Bars;Decoding;Cognition;Blindness;Accessibility;assistive technology;blind and visually impaired people;computer vision;data visualisation;document image analysis;image processing},
}

@article{26,
    key = {rayyan-4717662},
    title = {A deep learning system for recognizing and recovering contaminated slider serial numbers in hard disk manufacturing processes},
    year = {2021},
    journal = {Sensors},
    issn = {14248220 (ISSN)},
    volume = {21},
    authors = {Chousangsuntorn, C. and Tongloy, T. and Chuwongin, S. and Boonsang, S.},
    abstract = {This paper outlines a system for detecting printing errors and misidentifications on hard disk drive sliders, which may contribute to shipping tracking problems and incorrect product delivery to end users. A deep-learning-based technique is proposed for determining the printed identity of a slider serial number from images captured by a digital camera. Our approach starts with image preprocessing methods that deal with differences in lighting and printing positions and then progresses to deep learning character detection based on the You-Only-Look-Once (YOLO) v4 algorithm and finally character classification. For character classification, four convolutional neural networks (CNN) were compared for accuracy and effectiveness: DarkNet-19, EfficientNet-B0, ResNet-50, and DenseNet-201. Experimenting on almost 15,000 photographs yielded accuracy greater than 99% on four CNN networks, proving the feasibility of the proposed technique. The EfficientNet-B0 network outperformed highly qualified human readers with the best recovery rate (98.4%) and fastest inference time (256.91 ms). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
    notes = {Export Date: 10 May 2024; Cited By: 3; Correspondence Address: S. Boonsang; Department of Electrical Engineering, School of Engineering, Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, 10520, Thailand; email: siridech.bo@kmitl.ac.th | RAYYAN-INCLUSION: {"Jonatan"=>"Included"}},
    doi = {10.3390/s21186261},
    keywords = {Character classification;Convolution neural networks;Hard disk drive;Optical character recognition;Algorithms;Deep Learning;Humans;Neural Networks, Computer;Convolutional neural networks;Learning systems;Hard Disk Drive;Hard disk manufacturing;Image preprocessing;Printing errors;Product delivery;Recovery rate;Tracking problem;algorithm;human;Deep learning},
}

